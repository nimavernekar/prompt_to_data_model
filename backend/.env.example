# Override if Ollama isn't local
OLLAMA_HOST=http://localhost:11434

# LLaMA model for answer generation
LLM_MODEL=llama3.1:8b

# Embedding model for vector search
EMBED_MODEL=nomic-embed-text
